{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ViT Benchmark: Swin-Tiny\n",
                "## Phase 1: ASDID (Source) -> MH-Soya (Target)\n",
                "\n",
                "This notebook evaluates the performance of a Vision Transformer (Swin) under Test-Time Adaptation (TTA) using TENT and EATA.\n",
                "\n",
                "**Hypothesis**: Does the ViT baseline perform better than CNN? (Does global attention ignore the soil difference?)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "\n",
                "import torch\n",
                "import os\n",
                "import sys\n",
                "from tqdm.notebook import tqdm\n",
                "\n",
                "# Add root to path\n",
                "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
                "\n",
                "from src.dataset.loaders import get_dataloaders\n",
                "from src.model.architectures import get_model, configure_for_tta\n",
                "from src.utils.metrics import calculate_metrics\n",
                "from src.utils.plotting import plot_confusion_matrix, get_specific_errors, plot_specific_errors, plot_loss_curves\n",
                "from src.utils.logging import set_seed, log_experiment, log_training_progress\n",
                "from src.tent.optimizer import TentOptimizer\n",
                "from src.eata.fisher import compute_fisher_diagonal\n",
                "from src.eata.optimizer import EataOptimizer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup & Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "SEED = 21\n",
                "MODEL_NAME = \"swin_tiny\"\n",
                "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
                "DATA_ASDID = \"../data/ASDID\"\n",
                "DATA_MH = \"../data/MH-SoyaHealthVision/Soyabean_Leaf_Image_Dataset\"\n",
                "CLASS_NAMES = ['Healthy', 'Rust', 'Frogeye']\n",
                "\n",
                "set_seed(SEED)\n",
                "print(f\"Device: {DEVICE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Baseline Training on ASDID\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_loader_asdid, val_loader_asdid, test_loader_asdid, dataset_asdid, _, _, _ = get_dataloaders(\"ASDID\", DATA_ASDID, seed=SEED)\n",
                "train_loader_mh, _, test_loader_mh, _, _, _, _ = get_dataloaders(\"MH\", DATA_MH, seed=SEED)\n",
                "\n",
                "model = get_model(MODEL_NAME, num_classes=3).to(DEVICE)\n",
                "\n",
                "criterion = torch.nn.CrossEntropyLoss()\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
                "\n",
                "EPOCHS = 3\n",
                "best_val_f1 = 0\n",
                "train_losses, val_f1s = [], []\n",
                "\n",
                "for epoch in range(EPOCHS):\n",
                "    model.train()\n",
                "    epoch_loss = 0\n",
                "    for inputs, labels in tqdm(train_loader_asdid, desc=f\"Epoch {epoch+1}\", leave=False):\n",
                "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(inputs)\n",
                "        loss = criterion(outputs, labels)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        epoch_loss += loss.item()\n",
                "    \n",
                "    # Validation\n",
                "    model.eval()\n",
                "    all_preds, all_labels = [], []\n",
                "    with torch.no_grad():\n",
                "        for inputs, labels in val_loader_asdid:\n",
                "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
                "            outputs = model(inputs)\n",
                "            _, preds = torch.max(outputs, 1)\n",
                "            all_preds.extend(preds.cpu().numpy())\n",
                "            all_labels.extend(labels.cpu().numpy())\n",
                "    \n",
                "    metrics = calculate_metrics(all_labels, all_preds, CLASS_NAMES)\n",
                "    val_f1s.append(metrics['F1'])\n",
                "    train_losses.append(epoch_loss / len(train_loader_asdid))\n",
                "    \n",
                "    if metrics['F1'] > best_val_f1:\n",
                "        best_val_f1 = metrics['F1']\n",
                "        torch.save(model.state_dict(), f\"best_{MODEL_NAME}.pth\")\n",
                "\n",
                "plot_loss_curves(train_losses, val_f1s, title=f\"Baseline Training ({MODEL_NAME})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Evaluate Baseline on Target MH"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.load_state_dict(torch.load(f\"best_{MODEL_NAME}.pth\"))\n",
                "model.eval()\n",
                "\n",
                "all_preds, all_labels = [], []\n",
                "with torch.no_grad():\n",
                "    for inputs, labels in test_loader_mh:\n",
                "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
                "        outputs = model(inputs)\n",
                "        _, preds = torch.max(outputs, 1)\n",
                "        all_preds.extend(preds.cpu().numpy())\n",
                "        all_labels.extend(labels.cpu().numpy())\n",
                "\n",
                "metrics_baseline = calculate_metrics(all_labels, all_preds, CLASS_NAMES)\n",
                "plot_confusion_matrix(metrics_baseline['Confusion_Matrix'], CLASS_NAMES, title=f\"Baseline ViT -> MH\")\n",
                "log_experiment(f\"{MODEL_NAME}_Baseline\", SEED, MODEL_NAME, \"ASDID\", \"MH\", metrics_baseline, adaptation=\"None\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: TENT Adaptation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_tent = get_model(MODEL_NAME, num_classes=3).to(DEVICE)\n",
                "model_tent.load_state_dict(torch.load(f\"best_{MODEL_NAME}.pth\"))\n",
                "model_tent = configure_for_tta(model_tent, method='tent')\n",
                "\n",
                "tent_opt = TentOptimizer(model_tent, lr=1e-3, steps=1)\n",
                "model_tent = tent_opt.run_adaptation(train_loader_mh, DEVICE)\n",
                "\n",
                "# Evaluation\n",
                "model_tent.eval()\n",
                "all_preds, all_labels = [], []\n",
                "with torch.no_grad():\n",
                "    for inputs, labels in test_loader_mh:\n",
                "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
                "        outputs = model_tent(inputs)\n",
                "        _, preds = torch.max(outputs, 1)\n",
                "        all_preds.extend(preds.cpu().numpy())\n",
                "        all_labels.extend(labels.cpu().numpy())\n",
                "\n",
                "metrics_tent = calculate_metrics(all_labels, all_preds, CLASS_NAMES)\n",
                "plot_confusion_matrix(metrics_tent['Confusion_Matrix'], CLASS_NAMES, title=f\"TENT ViT -> MH\")\n",
                "log_experiment(f\"{MODEL_NAME}_TENT\", SEED, MODEL_NAME, \"ASDID\", \"MH\", metrics_tent, adaptation=\"TENT\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: EATA Adaptation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_eata = get_model(MODEL_NAME, num_classes=3).to(DEVICE)\n",
                "model_eata.load_state_dict(torch.load(f\"best_{MODEL_NAME}.pth\"))\n",
                "model_eata = configure_for_tta(model_eata, method='eata')\n",
                "\n",
                "print(\"Computing Fisher matrix on source data...\")\n",
                "fisher = compute_fisher_diagonal(model_eata, train_loader_asdid, DEVICE, num_samples=256)\n",
                "\n",
                "eata_opt = EataOptimizer(model_eata, fisher, lr=1e-3, steps=1)\n",
                "model_eata = eata_opt.run_adaptation(train_loader_mh, DEVICE)\n",
                "\n",
                "# Evaluation\n",
                "model_eata.eval()\n",
                "all_preds, all_labels = [], []\n",
                "with torch.no_grad():\n",
                "    for inputs, labels in test_loader_mh:\n",
                "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
                "        outputs = model_eata(inputs)\n",
                "        _, preds = torch.max(outputs, 1)\n",
                "        all_preds.extend(preds.cpu().numpy())\n",
                "        all_labels.extend(labels.cpu().numpy())\n",
                "\n",
                "metrics_eata = calculate_metrics(all_labels, all_preds, CLASS_NAMES)\n",
                "plot_confusion_matrix(metrics_eata['Confusion_Matrix'], CLASS_NAMES, title=f\"EATA ViT -> MH\")\n",
                "log_experiment(f\"{MODEL_NAME}_EATA\", SEED, MODEL_NAME, \"ASDID\", \"MH\", metrics_eata, adaptation=\"EATA\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visual Error Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n--- EATA Error Analysis: Frogeye as Rust ---\")\n",
                "errors = get_specific_errors(model_eata, test_loader_mh, DEVICE, CLASS_NAMES, \"Frogeye\", \"Rust\")\n",
                "plot_specific_errors(errors, title=\"EATA: Top Frogeye-predicted-as-Rust (ViT)\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
