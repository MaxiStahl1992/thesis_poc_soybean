{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b917bace",
   "metadata": {},
   "source": [
    "## Setup & Installation\n",
    "\n",
    "**Note**: Ultralytics is already installed in requirements.txt.\n",
    "\n",
    "SAM models will auto-download when first used:\n",
    "- **SAM 2 Base** (`sam_b.pt`): ~375MB - Recommended\n",
    "- **SAM 2 Large** (`sam_l.pt`): ~1.2GB - Higher quality\n",
    "- **SAM 3** (`sam3.pt`): Requires manual download from HuggingFace\n",
    "\n",
    "For this experiment, we'll use SAM 2 Base which auto-downloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fb8017e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Results directory: /Users/stahlma/Desktop/01_Studium/11_Thesis/soybean/thesis_poc/notebooks/results/sam_background_removal\n",
      "Data directory: /Users/stahlma/Desktop/01_Studium/11_Thesis/soybean/thesis_poc/data\n",
      "Using Ultralytics SAM - weights will auto-download if needed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('../'))\n",
    "\n",
    "from src.segmentation import (\n",
    "    load_sam_model,\n",
    "    segment_image,\n",
    "    get_largest_mask,\n",
    "    process_dataset,\n",
    "    create_segmented_dataset,\n",
    "    verify_dataset_structure\n",
    ")\n",
    "from src.segmentation.background_removal import (\n",
    "    process_image_with_sam,\n",
    "    visualize_background_removal\n",
    ")\n",
    "from src.dataset.loaders import get_dataloaders\n",
    "from src.utils.metrics import calculate_metrics\n",
    "\n",
    "SEED = 21\n",
    "DEVICE = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# Directories\n",
    "NOTEBOOK_DIR = Path(os.getcwd())\n",
    "RESULTS_DIR = NOTEBOOK_DIR / 'notebooks'/ 'results' / 'sam_background_removal'\n",
    "DATA_DIR = NOTEBOOK_DIR / 'data'\n",
    "MODELS_DIR = NOTEBOOK_DIR/ 'models'\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Using Ultralytics SAM - weights will auto-download if needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ed33ad",
   "metadata": {},
   "source": [
    "## Step 1: Load SAM Model\n",
    "\n",
    "Using Ultralytics SAM 2 - weights will auto-download (~375MB for Base model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f94d9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SAM model using Ultralytics...\n",
      "This will auto-download weights if not cached (~375MB).\n",
      "\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/sam2.1_b.pt to 'sam2.1_b.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 154.4MB 6.6MB/s 23.5s 23.5s<0.1s\n",
      "‚úÖ Loaded SAM model: sam2.1_b.pt\n",
      "\n",
      "‚úÖ SAM model loaded successfully!\n",
      "   Model: SAM 2 Base\n",
      "   Backend: Ultralytics\n",
      "   Ready for segmentation\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading SAM model using Ultralytics...\")\n",
    "print(\"This will auto-download weights if not cached (~375MB).\\n\")\n",
    "\n",
    "# Load SAM model - will auto-download if needed\n",
    "model = load_sam_model('sam2.1_b.pt')\n",
    "\n",
    "print(\"\\n‚úÖ SAM model loaded successfully!\")\n",
    "print(f\"   Model: SAM 2 Base\")\n",
    "print(f\"   Backend: Ultralytics\")\n",
    "print(f\"   Ready for segmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82beace9",
   "metadata": {},
   "source": [
    "## Step 2: Test SAM on Sample Images\n",
    "\n",
    "Visualize segmentation on a few sample images to verify it's working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "078da718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing SAM on 3 sample images...\n",
      "\n",
      "Processing: 20241006_181712.jpg\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 /Users/stahlma/Desktop/01_Studium/11_Thesis/soybean/thesis_poc/data/MH-SoyaHealthVision/Soyabean_Leaf_Image_Dataset/Soyabean_Rust/20241006_181712.jpg: 1024x1024 1 0, 1250.0ms\n",
      "Speed: 27.9ms preprocess, 1250.0ms inference, 8.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "  Masks found: 1\n",
      "  Mask coverage: 41.5%\n",
      "  Mask area: 3879069 pixels\n",
      "\n",
      "{'num_masks_found': 1, 'num_masks_merged': 1, 'mask_area': 3879069, 'mask_coverage': 0.4153563027671939}\n",
      "Processing: 20241006_084656.jpg\n",
      "\n",
      "image 1/1 /Users/stahlma/Desktop/01_Studium/11_Thesis/soybean/thesis_poc/data/MH-SoyaHealthVision/Soyabean_Leaf_Image_Dataset/Soyabean_Rust/20241006_084656.jpg: 1024x1024 1 0, 1141.6ms\n",
      "Speed: 7.9ms preprocess, 1141.6ms inference, 17.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "  Masks found: 1\n",
      "  Mask coverage: 20.8%\n",
      "  Mask area: 1938807 pixels\n",
      "\n",
      "{'num_masks_found': 1, 'num_masks_merged': 1, 'mask_area': 1938807, 'mask_coverage': 0.20760025338532387}\n",
      "Processing: 20240928_170413.jpg\n",
      "\n",
      "image 1/1 /Users/stahlma/Desktop/01_Studium/11_Thesis/soybean/thesis_poc/data/MH-SoyaHealthVision/Soyabean_Leaf_Image_Dataset/Soyabean_Rust/20240928_170413.jpg: 1024x1024 (no detections), 1120.8ms\n",
      "Speed: 6.8ms preprocess, 1120.8ms inference, 8.5ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stahlma/Desktop/01_Studium/11_Thesis/soybean/thesis_poc/venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/stahlma/Desktop/01_Studium/11_Thesis/soybean/thesis_poc/venv/lib/python3.12/site-packages/numpy/_core/_methods.py:145: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation minimum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Process with SAM (using center point heuristic)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m cleaned_image, mask, metadata = \u001b[43mprocess_image_with_sam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Masks found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetadata[\u001b[33m'\u001b[39m\u001b[33mnum_masks_found\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Mask coverage: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetadata[\u001b[33m'\u001b[39m\u001b[33mmask_coverage\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/soybean/thesis_poc/src/segmentation/background_removal.py:164\u001b[39m, in \u001b[36mprocess_image_with_sam\u001b[39m\u001b[34m(image_path, model, background_color, text_prompt, point_prompt, bbox_prompt, merge_threshold)\u001b[39m\n\u001b[32m    161\u001b[39m         metadata = {\u001b[33m'\u001b[39m\u001b[33mnum_masks_found\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmask_area\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmask_coverage\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0.0\u001b[39m}\n\u001b[32m    162\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m image, full_mask, metadata\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     stats = \u001b[43mget_mask_statistics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m     num_masks = stats[\u001b[33m'\u001b[39m\u001b[33mnum_masks\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    167\u001b[39m \u001b[38;5;66;03m# Filter and merge masks above threshold\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/soybean/thesis_poc/src/segmentation/sam_utils.py:378\u001b[39m, in \u001b[36mget_mask_statistics\u001b[39m\u001b[34m(results)\u001b[39m\n\u001b[32m    376\u001b[39m         confs = result.boxes.conf.cpu().numpy()\n\u001b[32m    377\u001b[39m         stats[\u001b[33m'\u001b[39m\u001b[33mmean_confidence\u001b[39m\u001b[33m'\u001b[39m] = np.mean(confs)\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m         stats[\u001b[33m'\u001b[39m\u001b[33mmin_confidence\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m         stats[\u001b[33m'\u001b[39m\u001b[33mmax_confidence\u001b[39m\u001b[33m'\u001b[39m] = np.max(confs)\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m stats\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/soybean/thesis_poc/venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:3302\u001b[39m, in \u001b[36mmin\u001b[39m\u001b[34m(a, axis, out, keepdims, initial, where)\u001b[39m\n\u001b[32m   3190\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_min_dispatcher)\n\u001b[32m   3191\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmin\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=np._NoValue, initial=np._NoValue,\n\u001b[32m   3192\u001b[39m         where=np._NoValue):\n\u001b[32m   3193\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3194\u001b[39m \u001b[33;03m    Return the minimum of an array or minimum along an axis.\u001b[39;00m\n\u001b[32m   3195\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3300\u001b[39m \u001b[33;03m    6\u001b[39;00m\n\u001b[32m   3301\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3302\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mminimum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmin\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3303\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/soybean/thesis_poc/venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:86\u001b[39m, in \u001b[36m_wrapreduction\u001b[39m\u001b[34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[39m\n\u001b[32m     83\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     84\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis=axis, out=out, **passkwargs)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: zero-size array to reduction operation minimum which has no identity"
     ]
    }
   ],
   "source": [
    "# Get sample images from MH\n",
    "mh_rust_dir = DATA_DIR / 'MH-SoyaHealthVision' / 'Soyabean_Leaf_Image_Dataset' / 'Soyabean_Rust'\n",
    "sample_images = list(mh_rust_dir.glob('*.[jJ][pP][gG]'))[:3]\n",
    "\n",
    "print(f\"Testing SAM on {len(sample_images)} sample images...\\n\")\n",
    "\n",
    "for i, img_path in enumerate(sample_images):\n",
    "    print(f\"Processing: {img_path.name}\")\n",
    "    \n",
    "    # Process with SAM (using center point heuristic)\n",
    "    cleaned_image, mask, metadata = process_image_with_sam(\n",
    "        str(img_path),\n",
    "        model,\n",
    "        background_color=(0, 0, 0)\n",
    "    )\n",
    "    \n",
    "    print(f\"  Masks found: {metadata['num_masks_found']}\")\n",
    "    print(f\"  Mask coverage: {metadata['mask_coverage']:.1%}\")\n",
    "    print(f\"  Mask area: {metadata['mask_area']} pixels\\n\")\n",
    "\n",
    "    print(metadata)\n",
    "    \n",
    "    # Visualize\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    vis = visualize_background_removal(\n",
    "        image, mask, cleaned_image,\n",
    "        save_path=RESULTS_DIR / f'sample_{i+1}_comparison.png'\n",
    "    )\n",
    "    if vis is not None:\n",
    "        plt.show()\n",
    "\n",
    "print(\"\\nüìä Sample visualizations saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87cacd5",
   "metadata": {},
   "source": [
    "## üî• Two-Stage Detection: YOLO + SAM\n",
    "\n",
    "Now let's test the **two-stage approach** (YOLO detection + SAM segmentation):\n",
    "\n",
    "1. **YOLO** detects all leaves in the image\n",
    "2. **SAM** segments each detected leaf\n",
    "3. All masks are merged for complete foreground extraction\n",
    "\n",
    "This approach handles:\n",
    "- ‚úÖ Multiple leaves in one image\n",
    "- ‚úÖ Partially visible leaves at edges\n",
    "- ‚úÖ Diseased leaves with color variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9d37a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample images with multiple leaves\n",
    "mh_rust_dir = DATA_DIR / 'MH-SoyaHealthVision' / 'Soyabean_Leaf_Image_Dataset' / 'Soyabean_Rust'\n",
    "sample_images = list(mh_rust_dir.glob('*.[jJ][pP][gG]'))[:3]\n",
    "\n",
    "print(f\"Testing Two-Stage Detection on {len(sample_images)} images...\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, img_path in enumerate(sample_images):\n",
    "    print(f\"\\nüì∏ Image {i+1}: {img_path.name}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Two-stage detection: YOLO + SAM\n",
    "    cleaned_image, mask, metadata = process_image_with_sam(\n",
    "        str(img_path),\n",
    "        model,\n",
    "        background_color=(0, 0, 0),\n",
    "        use_yolo_detection=True  # Enable two-stage detection\n",
    "    )\n",
    "    \n",
    "    print(f\"  üîç Stage 1 (YOLO): {metadata.get('num_leaves_detected', 'N/A')} leaves detected\")\n",
    "    print(f\"  üéØ Stage 2 (SAM): {metadata['num_masks_found']} masks found\")\n",
    "    print(f\"  üîó Stage 3 (Merge): {metadata['num_masks_merged']} masks merged\")\n",
    "    print(f\"  üìä Coverage: {metadata['mask_coverage']:.1%}\")\n",
    "    print(f\"  üìê Mask area: {metadata['mask_area']:,} pixels\")\n",
    "    print(f\"  üõ†Ô∏è  Method: {metadata.get('method', 'default')}\")\n",
    "    \n",
    "    # Visualize\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    vis = visualize_background_removal(\n",
    "        image, mask, cleaned_image,\n",
    "        save_path=RESULTS_DIR / f'two_stage_sample_{i+1}.png'\n",
    "    )\n",
    "    if vis is not None:\n",
    "        display(vis)\n",
    "        plt.close()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ Two-stage detection test complete!\")\n",
    "print(f\"üíæ Results saved to: {RESULTS_DIR}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d85149",
   "metadata": {},
   "source": [
    "### üìä Comparison: Single-Stage vs Two-Stage\n",
    "\n",
    "Let's compare both approaches on the same image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3661a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one test image\n",
    "test_img = sample_images[0]\n",
    "print(f\"Comparing approaches on: {test_img.name}\\n\")\n",
    "\n",
    "# 1. Single-stage (center point)\n",
    "print(\"1Ô∏è‚É£ Single-Stage SAM (center point)\")\n",
    "print(\"-\" * 50)\n",
    "cleaned_single, mask_single, meta_single = process_image_with_sam(\n",
    "    str(test_img),\n",
    "    model,\n",
    "    use_yolo_detection=False\n",
    ")\n",
    "print(f\"  ‚Ä¢ Masks found: {meta_single['num_masks_found']}\")\n",
    "print(f\"  ‚Ä¢ Masks merged: {meta_single['num_masks_merged']}\")\n",
    "print(f\"  ‚Ä¢ Coverage: {meta_single['mask_coverage']:.1%}\\n\")\n",
    "\n",
    "# 2. Two-stage (YOLO + SAM)\n",
    "print(\"2Ô∏è‚É£ Two-Stage YOLO + SAM\")\n",
    "print(\"-\" * 50)\n",
    "cleaned_two, mask_two, meta_two = process_image_with_sam(\n",
    "    str(test_img),\n",
    "    model,\n",
    "    use_yolo_detection=True\n",
    ")\n",
    "print(f\"  ‚Ä¢ Leaves detected: {meta_two.get('num_leaves_detected', 'N/A')}\")\n",
    "print(f\"  ‚Ä¢ Masks found: {meta_two['num_masks_found']}\")\n",
    "print(f\"  ‚Ä¢ Masks merged: {meta_two['num_masks_merged']}\")\n",
    "print(f\"  ‚Ä¢ Coverage: {meta_two['mask_coverage']:.1%}\\n\")\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "original = Image.open(test_img).convert('RGB')\n",
    "\n",
    "# Row 1: Single-stage\n",
    "axes[0, 0].imshow(original)\n",
    "axes[0, 0].set_title('Original Image', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(mask_single, cmap='gray')\n",
    "axes[0, 1].set_title(f'Single-Stage Mask\\n({meta_single[\"num_masks_merged\"]} masks)', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[0, 2].imshow(cleaned_single)\n",
    "axes[0, 2].set_title(f'Single-Stage Result\\n({meta_single[\"mask_coverage\"]*100:.1f}% coverage)', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# Row 2: Two-stage\n",
    "axes[1, 0].imshow(original)\n",
    "axes[1, 0].set_title('Original Image', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(mask_two, cmap='gray')\n",
    "axes[1, 1].set_title(f'Two-Stage Mask\\n({meta_two[\"num_masks_merged\"]} masks, ' +\n",
    "                     f'{meta_two.get(\"num_leaves_detected\", 0)} leaves)', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "axes[1, 2].imshow(cleaned_two)\n",
    "axes[1, 2].set_title(f'Two-Stage Result\\n({meta_two[\"mask_coverage\"]*100:.1f}% coverage)', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "# Add row labels\n",
    "fig.text(0.02, 0.75, '1. Single-Stage', rotation=90, fontsize=14, \n",
    "         fontweight='bold', va='center')\n",
    "fig.text(0.02, 0.25, '2. Two-Stage', rotation=90, fontsize=14, \n",
    "         fontweight='bold', va='center')\n",
    "\n",
    "plt.tight_layout(rect=[0.03, 0, 1, 1])\n",
    "plt.savefig(RESULTS_DIR / 'single_vs_two_stage_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Comparison saved to: {RESULTS_DIR / 'single_vs_two_stage_comparison.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cac22d2",
   "metadata": {},
   "source": [
    "## Step 3: Create Segmented MH Dataset\n",
    "\n",
    "Process the entire MH dataset and create `MH_Segmented` with backgrounds removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcf4562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "mh_source = DATA_DIR / 'MH-SoyaHealthVision' / 'Soyabean_Leaf_Image_Dataset'\n",
    "mh_output = DATA_DIR / 'MH_Segmented'\n",
    "\n",
    "# Class folders to process\n",
    "class_folders = [\n",
    "    'Healthy_Soyabean',\n",
    "    'Soyabean_Rust',\n",
    "    'Soyabean_Frog_Leaf_Eye'\n",
    "]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CREATING SEGMENTED MH DATASET\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Source: {mh_source}\")\n",
    "print(f\"Output: {mh_output}\")\n",
    "print(f\"Classes: {class_folders}\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚ö†Ô∏è This will take several minutes...\\n\")\n",
    "\n",
    "# Process dataset\n",
    "stats = create_segmented_dataset(\n",
    "    dataset_name='MH',\n",
    "    data_root=str(mh_source),\n",
    "    output_root=str(mh_output),\n",
    "    model=model,\n",
    "    background_color=(0, 0, 0),\n",
    "    class_folders=class_folders\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Segmented dataset created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99c0428",
   "metadata": {},
   "source": [
    "## Step 4: Verify Segmented Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c302780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify structure\n",
    "print(\"Verifying segmented dataset structure...\\n\")\n",
    "is_valid = verify_dataset_structure(str(mh_output))\n",
    "\n",
    "if is_valid:\n",
    "    print(\"\\n‚úÖ Dataset structure is valid!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Dataset structure is invalid!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7fb2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some comparisons\n",
    "from src.segmentation.batch_processing import compare_original_vs_segmented\n",
    "\n",
    "print(\"\\nCreating comparison visualizations...\\n\")\n",
    "\n",
    "for class_name in class_folders:\n",
    "    print(f\"Comparing: {class_name}\")\n",
    "    compare_original_vs_segmented(\n",
    "        original_dir=str(mh_source / class_name),\n",
    "        segmented_dir=str(mh_output / class_name),\n",
    "        num_samples=3,\n",
    "        save_path=RESULTS_DIR / f'{class_name}_comparison.png'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6f21ef",
   "metadata": {},
   "source": [
    "## Step 5: Load Baseline ResNet50\n",
    "\n",
    "Load the pre-trained model from ASDID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8284eb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to pre-trained model\n",
    "pretrained_model_path = NOTEBOOK_DIR.parent / 'notebooks' / 'results' / 'best_resnet50.pth'\n",
    "\n",
    "if not pretrained_model_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Pre-trained ResNet50 not found at {pretrained_model_path}. \"\n",
    "        \"Please run notebook 01_cnn_baseline.ipynb first.\"\n",
    "    )\n",
    "\n",
    "print(f\"‚úÖ Found pre-trained model: {pretrained_model_path}\")\n",
    "\n",
    "# Load model\n",
    "model_resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "model_resnet.fc = nn.Linear(model_resnet.fc.in_features, 3)\n",
    "model_resnet = model_resnet.to(DEVICE)\n",
    "\n",
    "# Load pre-trained weights\n",
    "checkpoint = torch.load(pretrained_model_path, map_location=DEVICE)\n",
    "if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "    model_resnet.load_state_dict(checkpoint['model_state_dict'])\n",
    "else:\n",
    "    model_resnet.load_state_dict(checkpoint)\n",
    "\n",
    "print(f\"‚úÖ Loaded source-trained ResNet50\")\n",
    "print(f\"Model: ResNet50 (25.6M params)\")\n",
    "print(f\"Source domain: ASDID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c13074c",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate on Original MH (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1693aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard evaluation transforms\n",
    "eval_transforms = T.Compose([\n",
    "    T.Resize(int(IMG_SIZE * 1.14)),\n",
    "    T.CenterCrop(IMG_SIZE),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load original MH dataset\n",
    "print(\"Loading original MH dataset...\")\n",
    "_, _, mh_test_loader, mh_dataset, _, _, _ = get_dataloaders(\n",
    "    dataset_name='MH',\n",
    "    data_root=str(mh_source),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    train_transform=eval_transforms,\n",
    "    test_transform=eval_transforms,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "print(f\"MH Test Set: {len(mh_test_loader.dataset)} images\")\n",
    "print(f\"Classes: {list(mh_dataset.class_to_idx.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5199e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device=DEVICE):\n",
    "    \"\"\"Evaluate model and return detailed results\"\"\"\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "    # Compute metrics\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    metrics = calculate_metrics(all_labels, all_preds)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    return metrics, all_labels, all_preds, cm\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Evaluating on ORIGINAL MH (No Background Removal)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "original_metrics, original_labels, original_preds, original_cm = evaluate_model(\n",
    "    model_resnet, mh_test_loader\n",
    ")\n",
    "\n",
    "print(f\"\\nOriginal MH Performance:\")\n",
    "print(f\"  Accuracy:  {original_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {original_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall:    {original_metrics['recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {original_metrics['f1']:.4f}\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(original_cm)\n",
    "\n",
    "# Store baseline for comparison\n",
    "baseline_f1 = original_metrics['f1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e973b6e0",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate on Segmented MH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615ae7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load segmented MH dataset\n",
    "print(\"Loading segmented MH dataset...\")\n",
    "_, _, mh_seg_test_loader, mh_seg_dataset, _, _, _ = get_dataloaders(\n",
    "    dataset_name='MH',\n",
    "    data_root=str(mh_output),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    train_transform=eval_transforms,\n",
    "    test_transform=eval_transforms,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "print(f\"MH Segmented Test Set: {len(mh_seg_test_loader.dataset)} images\")\n",
    "print(f\"Classes: {list(mh_seg_dataset.class_to_idx.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ed4cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"Evaluating on SEGMENTED MH (Background Removed)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "segmented_metrics, segmented_labels, segmented_preds, segmented_cm = evaluate_model(\n",
    "    model_resnet, mh_seg_test_loader\n",
    ")\n",
    "\n",
    "print(f\"\\nSegmented MH Performance:\")\n",
    "print(f\"  Accuracy:  {segmented_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {segmented_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall:    {segmented_metrics['recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {segmented_metrics['f1']:.4f}\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(segmented_cm)\n",
    "\n",
    "# Compare to baseline\n",
    "f1_improvement = segmented_metrics['f1'] - baseline_f1\n",
    "print(f\"\\nImprovement over baseline: {f1_improvement:+.4f} ({f1_improvement/baseline_f1*100:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e39330e",
   "metadata": {},
   "source": [
    "## Step 8: Comparison & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d13eb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Create summary table\n",
    "summary_data = {\n",
    "    'Method': ['Original MH', 'Segmented MH (SAM)'],\n",
    "    'F1 Score': [original_metrics['f1'], segmented_metrics['f1']],\n",
    "    'Accuracy': [original_metrics['accuracy'], segmented_metrics['accuracy']],\n",
    "    'Precision': [original_metrics['precision'], segmented_metrics['precision']],\n",
    "    'Recall': [original_metrics['recall'], segmented_metrics['recall']],\n",
    "    'F1 Improvement': [0.0, f1_improvement]\n",
    "}\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAM BACKGROUND REMOVAL - FINAL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(df_summary.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save summary\n",
    "df_summary.to_csv(RESULTS_DIR / 'sam_segmentation_summary.csv', index=False)\n",
    "print(f\"\\nüíæ Summary saved to: {RESULTS_DIR / 'sam_segmentation_summary.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e00f091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1. F1 Score Comparison\n",
    "methods = ['Original\\nMH', 'Segmented\\nMH (SAM)']\n",
    "f1_scores = [original_metrics['f1'], segmented_metrics['f1']]\n",
    "colors = ['#6C757D', '#28A745' if f1_improvement > 0 else '#DC3545']\n",
    "\n",
    "bars = axes[0].bar(methods, f1_scores, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "axes[0].set_ylabel('F1 Score', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('F1 Score Comparison', fontsize=13, fontweight='bold')\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add values on bars\n",
    "for bar, score in zip(bars, f1_scores):\n",
    "    height = bar.get_height()\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                f'{score:.4f}', ha='center', fontweight='bold', fontsize=11)\n",
    "\n",
    "# 2. Confusion Matrix - Original\n",
    "class_names = ['Frogeye', 'Healthy', 'Rust']  # Adjust based on actual order\n",
    "sns.heatmap(original_cm, annot=True, fmt='d', cmap='Blues', ax=axes[1],\n",
    "           xticklabels=class_names, yticklabels=class_names, cbar=True)\n",
    "axes[1].set_title(f'Original MH\\nF1: {original_metrics[\"f1\"]:.4f}', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('True Label')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "\n",
    "# 3. Confusion Matrix - Segmented\n",
    "sns.heatmap(segmented_cm, annot=True, fmt='d', cmap='Greens', ax=axes[2],\n",
    "           xticklabels=class_names, yticklabels=class_names, cbar=True)\n",
    "axes[2].set_title(f'Segmented MH (SAM)\\nF1: {segmented_metrics[\"f1\"]:.4f}', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylabel('True Label')\n",
    "axes[2].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'sam_segmentation_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c257ace",
   "metadata": {},
   "source": [
    "## Step 9: Analysis & Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cbda80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Performance Comparison:\")\n",
    "print(f\"   Original MH:    F1 = {original_metrics['f1']:.4f}\")\n",
    "print(f\"   Segmented MH:   F1 = {segmented_metrics['f1']:.4f}\")\n",
    "print(f\"   Improvement:    {f1_improvement:+.4f} ({f1_improvement/baseline_f1*100:+.1f}%)\")\n",
    "\n",
    "print(f\"\\nüéØ Hypothesis Testing:\")\n",
    "if f1_improvement > 0.10:\n",
    "    print(\"   ‚úÖ HYPOTHESIS STRONGLY CONFIRMED\")\n",
    "    print(\"   ‚Üí Background is a MAJOR source of domain shift\")\n",
    "    print(\"   ‚Üí SAM-based segmentation provides dramatic improvement\")\n",
    "    print(\"   ‚Üí Background removal should be a standard preprocessing step\")\n",
    "elif f1_improvement > 0.05:\n",
    "    print(\"   ‚úÖ HYPOTHESIS CONFIRMED\")\n",
    "    print(\"   ‚Üí Background contributes significantly to domain shift\")\n",
    "    print(\"   ‚Üí SAM segmentation provides measurable benefit\")\n",
    "    print(\"   ‚Üí Consider combining with other techniques\")\n",
    "elif f1_improvement > 0:\n",
    "    print(\"   ‚ö†Ô∏è HYPOTHESIS PARTIALLY CONFIRMED\")\n",
    "    print(\"   ‚Üí Background has modest impact on domain shift\")\n",
    "    print(\"   ‚Üí Other factors (lesion appearance, lighting) may dominate\")\n",
    "    print(\"   ‚Üí Consider hybrid approaches\")\n",
    "else:\n",
    "    print(\"   ‚ùå HYPOTHESIS NOT CONFIRMED\")\n",
    "    print(\"   ‚Üí Background is NOT the primary confounder\")\n",
    "    print(\"   ‚Üí Domain shift is driven by other factors\")\n",
    "    print(\"   ‚Üí Focus on feature-level or semantic adaptation\")\n",
    "\n",
    "# Analyze confusion matrix changes\n",
    "print(f\"\\nüìà Confusion Matrix Analysis:\")\n",
    "for i in range(len(class_names)):\n",
    "    orig_correct = original_cm[i, i]\n",
    "    seg_correct = segmented_cm[i, i]\n",
    "    orig_total = original_cm[i, :].sum()\n",
    "    seg_total = segmented_cm[i, :].sum()\n",
    "    \n",
    "    orig_acc = orig_correct / orig_total if orig_total > 0 else 0\n",
    "    seg_acc = seg_correct / seg_total if seg_total > 0 else 0\n",
    "    \n",
    "    improvement = seg_acc - orig_acc\n",
    "    \n",
    "    print(f\"   {class_names[i]}:\")\n",
    "    print(f\"      Original: {orig_acc:.1%} ({orig_correct}/{orig_total})\")\n",
    "    print(f\"      Segmented: {seg_acc:.1%} ({seg_correct}/{seg_total})\")\n",
    "    print(f\"      Change: {improvement:+.1%}\")\n",
    "\n",
    "print(f\"\\nüí° Thesis Implications:\")\n",
    "if f1_improvement > 0.05:\n",
    "    print(\"   ‚Ä¢ SAM-based preprocessing is effective for cross-domain transfer\")\n",
    "    print(\"   ‚Ä¢ Background removal should be standard in deployment pipelines\")\n",
    "    print(\"   ‚Ä¢ Consider combining with few-shot learning or input alignment\")\n",
    "    print(f\"   ‚Ä¢ ROI: {f1_improvement:.4f} F1 gain for one-time segmentation cost\")\n",
    "else:\n",
    "    print(\"   ‚Ä¢ Background is not the primary domain shift factor\")\n",
    "    print(\"   ‚Ä¢ Focus on disease-specific features and lesion appearance\")\n",
    "    print(\"   ‚Ä¢ Consider fine-tuning with labeled target data\")\n",
    "\n",
    "print(f\"\\nüî¨ Next Steps:\")\n",
    "if f1_improvement > 0.05:\n",
    "    print(\"   1. Combine SAM segmentation with few-shot learning (notebook 09)\")\n",
    "    print(\"   2. Test SAM + Input alignment (notebook 08) hybrid\")\n",
    "    print(\"   3. Deploy SAM preprocessing in production pipeline\")\n",
    "    print(\"   4. Test on other domain pairs (UAV data, different locations)\")\n",
    "else:\n",
    "    print(\"   1. Analyze failure cases: which images still fail after segmentation?\")\n",
    "    print(\"   2. Investigate lesion appearance differences (texture, color)\")\n",
    "    print(\"   3. Consider few-shot learning as primary solution\")\n",
    "    print(\"   4. Test domain-specific fine-tuning\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43a53b2",
   "metadata": {},
   "source": [
    "## Optional: Failure Case Analysis\n",
    "\n",
    "Analyze images where segmentation didn't help or made things worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffb9fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find cases where prediction changed from correct to incorrect\n",
    "orig_correct = np.array(original_labels) == np.array(original_preds)\n",
    "seg_correct = np.array(segmented_labels) == np.array(segmented_preds)\n",
    "\n",
    "# Cases that got worse\n",
    "worse_indices = np.where(orig_correct & ~seg_correct)[0]\n",
    "print(f\"\\nFound {len(worse_indices)} cases where segmentation made predictions worse\")\n",
    "\n",
    "# Cases that improved\n",
    "better_indices = np.where(~orig_correct & seg_correct)[0]\n",
    "print(f\"Found {len(better_indices)} cases where segmentation improved predictions\")\n",
    "\n",
    "print(f\"\\nNet improvement: {len(better_indices) - len(worse_indices)} images\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
