{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Soybean TTA: Comprehensive Results Summary\n",
                "\n",
                "## Overview\n",
                "\n",
                "This notebook aggregates and visualizes results from all TTA experiments:\n",
                "- **2 Base Models**: ConvNeXt Tiny, ViT Base Patch16\n",
                "- **5 TTA Methods**: BN+Pseudo, MEMO, TENT, EATA, Ensemble\n",
                "- **Cross-domain task**: ASDID (source) â†’ MH (target)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "\n",
                "sys.path.insert(0, os.path.abspath('../'))\n",
                "\n",
                "from src.utils.plotting import plot_confusion_matrix_grid\n",
                "\n",
                "RESULTS_DIR = Path('results')\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Experiment Registry"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load all experiment results\n",
                "registry_path = RESULTS_DIR / 'experiment_registry.csv'\n",
                "df = pd.read_csv(registry_path)\n",
                "\n",
                "print(f\"Total experiments logged: {len(df)}\")\n",
                "print(f\"\\nExperiment breakdown:\")\n",
                "print(df.groupby(['model', 'dataset', 'method']).size())\n",
                "\n",
                "# Display full registry\n",
                "df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Filter MH Cross-Domain Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Filter for MH cross-domain results only\n",
                "mh_results = df[df['dataset'] == 'MH'].copy()\n",
                "\n",
                "# Convert to numeric\n",
                "for col in ['accuracy', 'precision', 'recall', 'f1']:\n",
                "    mh_results[col] = pd.to_numeric(mh_results[col])\n",
                "\n",
                "# Display\n",
                "mh_results[['model', 'method', 'accuracy', 'precision', 'recall', 'f1']].sort_values(['model', 'f1'], ascending=[True, False])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Comparison Table: CNN vs ViT"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Pivot table for easy comparison\n",
                "comparison = mh_results.pivot_table(\n",
                "    index='method',\n",
                "    columns='model',\n",
                "    values=['f1', 'accuracy'],\n",
                "    aggfunc='mean'\n",
                ").round(4)\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"F1 Score Comparison: ConvNeXt Tiny vs ViT Base Patch16\".center(80))\n",
                "print(\"=\"*80)\n",
                "print(comparison['f1'])\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"Accuracy Comparison\".center(80))\n",
                "print(\"=\"*80)\n",
                "print(comparison['accuracy'])\n",
                "\n",
                "comparison"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## F1 Score Comparison Chart"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create grouped bar chart\n",
                "fig, ax = plt.subplots(figsize=(14, 7))\n",
                "\n",
                "# Prepare data\n",
                "methods = mh_results['method'].unique()\n",
                "cnn_f1s = [mh_results[(mh_results['method'] == m) & (mh_results['model'] == 'convnext_tiny')]['f1'].values[0] if len(mh_results[(mh_results['method'] == m) & (mh_results['model'] == 'convnext_tiny')]) > 0 else 0 for m in methods]\n",
                "vit_f1s = [mh_results[(mh_results['method'] == m) & (mh_results['model'] == 'vit_b_16')]['f1'].values[0] if len(mh_results[(mh_results['method'] == m) & (mh_results['model'] == 'vit_b_16')]) > 0 else 0 for m in methods]\n",
                "\n",
                "x = np.arange(len(methods))\n",
                "width = 0.35\n",
                "\n",
                "bars1 = ax.bar(x - width/2, cnn_f1s, width, label='ConvNeXt Tiny', color='#2E86AB', alpha=0.8)\n",
                "bars2 = ax.bar(x + width/2, vit_f1s, width, label='ViT Base Patch16', color='#A23B72', alpha=0.8)\n",
                "\n",
                "# Add value labels\n",
                "for bars in [bars1, bars2]:\n",
                "    for bar in bars:\n",
                "        height = bar.get_height()\n",
                "        if height > 0:\n",
                "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
                "                    f'{height:.4f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
                "\n",
                "ax.set_xlabel('TTA Method', fontsize=12, fontweight='bold')\n",
                "ax.set_ylabel('F1 Score', fontsize=12, fontweight='bold')\n",
                "ax.set_title('F1 Score Comparison: ConvNeXt vs ViT Across TTA Methods', fontsize=14, fontweight='bold', pad=20)\n",
                "ax.set_xticks(x)\n",
                "ax.set_xticklabels(methods, rotation=45, ha='right')\n",
                "ax.legend(fontsize=11)\n",
                "ax.grid(True, alpha=0.3, axis='y')\n",
                "ax.set_ylim(0, max(max(cnn_f1s), max(vit_f1s)) * 1.15)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(RESULTS_DIR / 'cnn_vit_tta_comparison.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## F1 Gains from Baseline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate F1 gains\n",
                "cnn_baseline = mh_results[(mh_results['model'] == 'convnext_tiny') & (mh_results['method'] == 'baseline')]['f1'].values[0]\n",
                "vit_baseline = mh_results[(mh_results['model'] == 'vit_b_16') & (mh_results['method'] == 'baseline')]['f1'].values[0]\n",
                "\n",
                "mh_results['f1_gain'] = mh_results.apply(\n",
                "    lambda row: row['f1'] - cnn_baseline if row['model'] == 'convnext_tiny' else row['f1'] - vit_baseline,\n",
                "    axis=1\n",
                ")\n",
                "\n",
                "# Display gains\n",
                "gains_df = mh_results[mh_results['method'] != 'baseline'][['model', 'method', 'f1', 'f1_gain']].sort_values(['model', 'f1_gain'], ascending=[True, False])\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"F1 Gains from Baseline\".center(70))\n",
                "print(\"=\"*70)\n",
                "print(f\"ConvNeXt Baseline: {cnn_baseline:.4f}\")\n",
                "print(f\"ViT Baseline:      {vit_baseline:.4f}\")\n",
                "print(\"=\"*70)\n",
                "gains_df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## F1 Gains Visualization)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "# CNN gains\n",
                "cnn_gains = mh_results[(mh_results['model'] == 'convnext_tiny') & (mh_results['method'] != 'baseline')]\n",
                "colors_cnn = ['green' if g > 0 else 'red' for g in cnn_gains['f1_gain']]\n",
                "\n",
                "ax1.barh(cnn_gains['method'], cnn_gains['f1_gain'], color=colors_cnn, alpha=0.7, edgecolor='black')\n",
                "ax1.axvline(x=0, color='black', linestyle='--', linewidth=2)\n",
                "ax1.set_xlabel('F1 Gain', fontsize=12, fontweight='bold')\n",
                "ax1.set_title('ConvNeXt Tiny: F1 Gains from Baseline', fontsize=13, fontweight='bold')\n",
                "ax1.grid(True, alpha=0.3, axis='x')\n",
                "\n",
                "# ViT gains\n",
                "vit_gains = mh_results[(mh_results['model'] == 'vit_b_16') & (mh_results['method'] != 'baseline')]\n",
                "colors_vit = ['green' if g > 0 else 'red' for g in vit_gains['f1_gain']]\n",
                "\n",
                "ax2.barh(vit_gains['method'], vit_gains['f1_gain'], color=colors_vit, alpha=0.7, edgecolor='black')\n",
                "ax2.axvline(x=0, color='black', linestyle='--', linewidth=2)\n",
                "ax2.set_xlabel('F1 Gain', fontsize=12, fontweight='bold')\n",
                "ax2.set_title('ViT Base Patch16: F1 Gains from Baseline', fontsize=13, fontweight='bold')\n",
                "ax2.grid(True, alpha=0.3, axis='x')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(RESULTS_DIR / 'f1_gains_comparison.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Best Performing Methods"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find best methods for each model\n",
                "cnn_best = mh_results[mh_results['model'] == 'convnext_tiny'].nlargest(1, 'f1')\n",
                "vit_best = mh_results[mh_results['model'] == 'vit_b_16'].nlargest(1, 'f1')\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"ðŸ† BEST PERFORMING METHODS\".center(80))\n",
                "print(\"=\"*80)\n",
                "\n",
                "print(\"\\nConvNeXt Tiny:\")\n",
                "print(f\"  Method: {cnn_best['method'].values[0]}\")\n",
                "print(f\"  F1 Score: {cnn_best['f1'].values[0]:.4f}\")\n",
                "print(f\"  Gain from baseline: +{cnn_best['f1_gain'].values[0]:.4f}\")\n",
                "\n",
                "print(\"\\nViT Base Patch16:\")\n",
                "print(f\"  Method: {vit_best['method'].values[0]}\")\n",
                "print(f\"  F1 Score: {vit_best['f1'].values[0]:.4f}\")\n",
                "print(f\"  Gain from baseline: +{vit_best['f1_gain'].values[0]:.4f}\")\n",
                "\n",
                "# Overall best\n",
                "overall_best = mh_results.nlargest(1, 'f1')\n",
                "print(\"\\n\" + \"-\"*80)\n",
                "print(\"ðŸ¥‡ OVERALL BEST CONFIGURATION\".center(80))\n",
                "print(\"-\"*80)\n",
                "print(f\"  Model: {overall_best['model'].values[0]}\")\n",
                "print(f\"  Method: {overall_best['method'].values[0]}\")\n",
                "print(f\"  F1 Score: {overall_best['f1'].values[0]:.4f}\")\n",
                "print(f\"  Accuracy: {overall_best['accuracy'].values[0]:.4f}\")\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Method Rankings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Rank methods by average F1 across both models\n",
                "method_avg = mh_results.groupby('method').agg({\n",
                "    'f1': ['mean', 'std'],\n",
                "    'accuracy': 'mean'\n",
                "}).round(4)\n",
                "\n",
                "method_avg.columns = ['F1 Mean', 'F1 Std', 'Accuracy Mean']\n",
                "method_avg = method_avg.sort_values('F1 Mean', ascending=False)\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"Method Rankings (Averaged Across Models)\".center(60))\n",
                "print(\"=\"*60)\n",
                "method_avg"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Export Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export comprehensive summary\n",
                "summary = {\n",
                "    'experiment_summary': {\n",
                "        'total_experiments': len(df),\n",
                "        'models': df['model'].unique().tolist(),\n",
                "        'datasets': df['dataset'].unique().tolist(),\n",
                "        'methods': df['method'].unique().tolist()\n",
                "    },\n",
                "    'baselines': {\n",
                "        'convnext_asdid': df[(df['model'] == 'convnext_tiny') & (df['dataset'] == 'ASDID') & (df['method'] == 'baseline')]['f1'].values[0] if len(df[(df['model'] == 'convnext_tiny') & (df['dataset'] == 'ASDID') & (df['method'] == 'baseline')]) > 0 else None,\n",
                "        'convnext_mh': cnn_baseline,\n",
                "        'vit_asdid': df[(df['model'] == 'vit_b_16') & (df['dataset'] == 'ASDID') & (df['method'] == 'baseline')]['f1'].values[0] if len(df[(df['model'] == 'vit_b_16') & (df['dataset'] == 'ASDID') & (df['method'] == 'baseline')]) > 0 else None,\n",
                "        'vit_mh': vit_baseline\n",
                "    },\n",
                "    'best_tta': {\n",
                "        'convnext': {\n",
                "            'method': cnn_best['method'].values[0],\n",
                "            'f1': float(cnn_best['f1'].values[0]),\n",
                "            'gain': float(cnn_best['f1_gain'].values[0])\n",
                "        },\n",
                "        'vit': {\n",
                "            'method': vit_best['method'].values[0],\n",
                "            'f1': float(vit_best['f1'].values[0]),\n",
                "            'gain': float(vit_best['f1_gain'].values[0])\n",
                "        },\n",
                "        'overall': {\n",
                "            'model': overall_best['model'].values[0],\n",
                "            'method': overall_best['method'].values[0],\n",
                "            'f1': float(overall_best['f1'].values[0])\n",
                "        }\n",
                "    }\n",
                "}\n",
                "\n",
                "import json\n",
                "with open(RESULTS_DIR / 'cnn_vit_tta_summary.json', 'w') as f:\n",
                "    json.dump(summary, f, indent=2)\n",
                "\n",
                "print(\"âœ… Summary exported to cnn_vit_tta_summary.json\")\n",
                "print(json.dumps(summary, indent=2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Key Findings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"#\"*80)\n",
                "print(\"# KEY FINDINGS\".ljust(79) + \"#\")\n",
                "print(\"#\"*80)\n",
                "\n",
                "print(\"\\n1. BASELINE PERFORMANCE:\")\n",
                "print(f\"   - ConvNeXt: ASDID F1={summary['baselines']['convnext_asdid']:.4f} â†’ MH F1={cnn_baseline:.4f} (drop: {summary['baselines']['convnext_asdid'] - cnn_baseline:.4f})\")\n",
                "print(f\"   - ViT:      ASDID F1={summary['baselines']['vit_asdid']:.4f} â†’ MH F1={vit_baseline:.4f} (drop: {summary['baselines']['vit_asdid'] - vit_baseline:.4f})\")\n",
                "\n",
                "print(\"\\n2. TTA EFFECTIVENESS:\")\n",
                "print(f\"   - ConvNeXt best: {summary['best_tta']['convnext']['method']} (+{summary['best_tta']['convnext']['gain']:.4f} F1)\")\n",
                "print(f\"   - ViT best:      {summary['best_tta']['vit']['method']} (+{summary['best_tta']['vit']['gain']:.4f} F1)\")\n",
                "\n",
                "print(\"\\n3. OVERALL WINNER:\")\n",
                "print(f\"   - {summary['best_tta']['overall']['model']} + {summary['best_tta']['overall']['method']}: F1={summary['best_tta']['overall']['f1']:.4f}\")\n",
                "\n",
                "print(\"\\n4. METHOD INSIGHTS:\")\n",
                "for idx, (method, row) in enumerate(method_avg.iterrows(), 1):\n",
                "    print(f\"   {idx}. {method}: Avg F1={row['F1 Mean']:.4f} Â± {row['F1 Std']:.4f}\")\n",
                "\n",
                "print(\"\\n\" + \"#\"*80)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}